
Wegen Ã„nderungen an MODULEPATH wurden folgende Module erneut geladen:
  1) cuda/12.4.1

[0;33m+-----------------------------------------------------------------------------------+
[0;33m|                       [1;31mCOMPLIANCE WITH SOFTWARE LICENSE TERMS[0;33m                      |
[0;33m|                                                                                   |
[0;33m| Your use of this module constitutes acceptance of the software license agreement. |
[0;33m| Read the Term of Service of Anaconda. In most cases you need to buy a license to  |
[0;33m| use the service. You might want to use the '[0;36mminiforge3[0;33m' module instead or set     |
[0;33m| another default cannel: [0;36mconda config --add channels conda-forge [0;33m                  |
[0;33m| [0;36mconda config --set channel_priority strict [0;33m     (both commands are necessary)     |
[0;33m|                                                                                   |
[0;33m| Alternatively use: [0;36mconda install [0m<package> [0;36m--channel conda-forge -n [0m<myenv>     [0;33m  |
[0;33m|                                                                                   |
[0;33m| '[1;31mFor sake of clarity, use by government entities and nonprofit entities with over[0;33m |
[0;33m| [1;31m200 employees or contractors is considered Organizational Use.                   [0;33m |
[0;33m| [1;31mPurchasing Starter tier license(s) does not satisfy the Organizational Use paid  [0;33m |
[0;33m| [1;31mlicense requirement set forth in this Section 2.1. Educational Entities will be  [0;33m |
[0;33m| [1;31mexempt from the paid license requirement, provided that the use of the Anaconda  [0;33m |
[0;33m| [1;31mOffering(s) is solely limited to being used for a curriculum-based course.       [0;33m |
[0;33m| [1;31mAnaconda reserves the right to monitor the registration, download, use,          [0;33m |
[0;33m| [1;31minstallation, access, or enjoyment of the Anaconda Offerings to ensure it is part[0;33m |
[0;33m| [1;31mof a curriculum.  Utilizing Miniconda to pull package updates from the           [0;33m |
[0;33m| [1;31mAnaconda Public Repository without a commercial license (if required by the      [0;33m |
[0;33m| [1;31mconditions set forth in Section 2 of this Terms of Service) is considered a      [0;33m |
[0;33m| [1;31mviolation of the Terms of Service.[0;33m'                                               |
[0;33m| [1;31m                  (ANACONDA TERMS OF SERVICE 2.1, Effective Date: March 31, 2024)[0;33m |
[0;33m+-----------------------------------------------------------------------------------+

[1;31mWARNING:[0;33m Deprecated module. Will be removed in April 2025.
Use the module '[0;36mminiforge3[0;33m' instead.

[1;31mWARNING:[0;33m Before switching to another conda providing module, you might want to reverse previous made changes to your shell by executing: [0;36mconda init --reverse[0;33m
Be aware that all conda environments share the same configuration files and folders.
[0m
# conda environments:
#
bla                      /beegfs/home/e/eilermas/.conda/envs/bla
env_torch                /beegfs/home/e/eilermas/.conda/envs/env_torch
install_env              /beegfs/home/e/eilermas/.conda/envs/install_env
kiaaa_env                /beegfs/home/e/eilermas/.conda/envs/kiaaa_env
need_env                 /beegfs/home/e/eilermas/.conda/envs/need_env
new                      /beegfs/home/e/eilermas/.conda/envs/new
obj_detection_env        /beegfs/home/e/eilermas/.conda/envs/obj_detection_env
paisenv                  /beegfs/home/e/eilermas/.conda/envs/paisenv
sap                      /beegfs/home/e/eilermas/.conda/envs/sap
torch_kaolin             /beegfs/home/e/eilermas/.conda/envs/torch_kaolin
base                  *  /cluster/anaconda3/2021.11

INFO 04-04 19:06:57 [__init__.py:239] Automatically detected platform cuda.
INFO 04-04 19:07:06 [config.py:585] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 04-04 19:07:06 [config.py:1519] Defaulting to use mp for distributed inference
INFO 04-04 19:07:06 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 04-04 19:07:07 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='/beegfs/home/e/eilermas/Projekte/models/DeepSeek-R1-Distill-Llama-70B', speculative_config=None, tokenizer='/beegfs/home/e/eilermas/Projekte/models/DeepSeek-R1-Distill-Llama-70B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/beegfs/home/e/eilermas/Projekte/models/DeepSeek-R1-Distill-Llama-70B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 04-04 19:07:07 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 60 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 04-04 19:07:07 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 10485760, 10, 'psm_218b430d'), local_subscribe_addr='ipc:///scratch-local/484762/3e504dce-2543-49a3-8429-9f786f4597a6', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 04-04 19:07:08 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x147ef9c6da80>
[1;36m(VllmWorker rank=0 pid=20240)[0;0m INFO 04-04 19:07:08 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d60a1f58'), local_subscribe_addr='ipc:///scratch-local/484762/f082e01d-a64e-41d5-bbbd-91dc4a3365ff', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 04-04 19:07:09 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x147ef9c6e1d0>
[1;36m(VllmWorker rank=1 pid=20257)[0;0m INFO 04-04 19:07:09 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_927c3f11'), local_subscribe_addr='ipc:///scratch-local/484762/abcb6840-38eb-418e-8611-667764189bd3', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 04-04 19:07:09 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x147ef9c6dba0>
[1;36m(VllmWorker rank=2 pid=20292)[0;0m INFO 04-04 19:07:09 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1e8e57ed'), local_subscribe_addr='ipc:///scratch-local/484762/10b700a4-5163-4d47-b3d5-a3a254b7f177', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 04-04 19:07:10 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x147ef9c6d030>
[1;36m(VllmWorker rank=3 pid=20319)[0;0m INFO 04-04 19:07:10 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7b85a909'), local_subscribe_addr='ipc:///scratch-local/484762/da8cd6ca-ac15-4c8a-b99c-4f254cd4518f', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=3 pid=20319)[0;0m INFO 04-04 19:07:10 [utils.py:931] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=3 pid=20319)[0;0m INFO 04-04 19:07:10 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=0 pid=20240)[0;0m INFO 04-04 19:07:10 [utils.py:931] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=20240)[0;0m INFO 04-04 19:07:10 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=2 pid=20292)[0;0m INFO 04-04 19:07:10 [utils.py:931] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=2 pid=20292)[0;0m INFO 04-04 19:07:10 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=1 pid=20257)[0;0m INFO 04-04 19:07:10 [utils.py:931] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=20257)[0;0m INFO 04-04 19:07:10 [pynccl.py:69] vLLM is using nccl==2.21.5
